{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this hands on you will build a model that once trained on a peice of text data can generate its own sequnce of words in a similar fashion as in trained data.\n",
    "\n",
    "- Follow the instructions provided for each cell and and code accordingly. \n",
    "- In order to run the cell press shift+enter.\n",
    "- make sure you have run all the cells before submitting the hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text data from story.txt file and split the text into seperate tokens, assign thr array of tokens to variable *training_data*\n",
    "\n",
    "### Expected output:\n",
    "    array(['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general',\n",
    "       'council', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code here\n",
    "\n",
    "\n",
    "\n",
    "training_data = \n",
    "###End code\n",
    "training_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the unique tokens in training_data nas sort them alphabetical order and assign the sorted list to variable words\n",
    "### create dictionary ind_to_word to map index to word\n",
    "### create another dictionary word_to_ind to reverse map word to their respective index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Start code here\n",
    "words = \n",
    "ind_to_word = \n",
    "word_to_ind = \n",
    "###End code\n",
    "print(\"words: \", words[:10], \"\\n\")\n",
    "print(\"index_to_words: \", list(ind_to_word.items())[:10], \"\\n\")\n",
    "print(\"word_to_index: \", list(word_to_ind.items())[:10], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write a function to generate training dataset \n",
    "    - parameters: dataset: orginal dataset\n",
    "                  look_back: the window size that tells the number of previous values in the series to look for to                   predict the next one.\n",
    "    - returns: feature and target arrays\n",
    "    \n",
    "example: \n",
    "         for window size 1:\n",
    "         dataset = [1,2,3,4]  \n",
    "         feature = [[1],[2],[3]]    \n",
    "         target = [2,3,4]  \n",
    "         \n",
    "         for window size 2:\n",
    "         feature = [[1,2],[2,3]]  \n",
    "         target = [3,4]  \n",
    "### Expected output when you when you call generate_dataset on training_data and look_back = 10 :\n",
    "input:  [[48, 5, 0, 85, 56, 37, 3, 35, 28, 92], [5, 0, 85, 56, 37, 3, 35, 28, 92, 25], [0, 85, 56, 37, 3, 35, 28, 92, 25, 102]]  \n",
    "labels:  [25, 102, 53]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Start code here\n",
    "def generate_dataset(dataset, look_back=10):\n",
    "    \n",
    "\n",
    "    \n",
    "###End code\n",
    "\n",
    "inputs, labels = generate_dataset(training_data, 10)\n",
    "print(\"input: \", inputs[:3])\n",
    "print(\"labels: \", labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step is to  reshape the inputs and normalize them.\n",
    "    - This step is coded for you\n",
    "    - Run the below cell to prepare the data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 10\n",
    "X_modified = np.reshape(inputs, (len(inputs), look_back, 1))\n",
    "X_modified = X_modified / float(len(words))\n",
    "Y_modified = np_utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using keras Sequential() class create a model having two LSTM block and one fully connected  layer with activation softmax\n",
    "### apply dropout with probability p = 0.2 between the layers of LSTM\n",
    "### compile the model with categorical_crossentropy loss and  adam optimizer\n",
    "\n",
    "### Expected output\n",
    "<img src=\"lstm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(51)\n",
    "###Start code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###End code\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model.fit() on train data with features as X_modified and target as Y_modified for 50 epoches and batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Start code here\n",
    "train_logs = model.fit(  ) \n",
    "###End code\n",
    "with open(\"output.txt\", \"w+\") as file:\n",
    "    file.write(\"train score {0:.2f}\\n\".format(train_logs.history[\"loss\"][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below codes takes a random sequence of words and generates more sequnce using the model you trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_mapped = inputs[50].copy()\n",
    "full_string = [ind_to_word[value] for value in string_mapped]\n",
    "# generating characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(words))\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "    seq = [ind_to_word[value] for value in string_mapped]\n",
    "    full_string.append(ind_to_word[pred_index])\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]\n",
    "\n",
    "    \n",
    "txt=\"\"\n",
    "for word in full_string:\n",
    "    txt = txt+\" \"+word\n",
    "print(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
